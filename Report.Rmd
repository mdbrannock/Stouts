---
title: "This Time With Stouts"
author: "Daniel Brannock"
date: "1/27/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
library(knitr)
library(tidyverse)
library(magrittr)

options(stringsAsFactors = F)
```

```{r data entry}
# Record the data. Each person tasted 22 samples.
raw <- data.frame(
  pos = 1:22,
  beerid = c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k",
             "f", "h", "j", "a", "c", "b", "d", "g", "k", "i", "e"),
  vic = c(7,3,5,6,2,4,5,3,6,1,5,6,2,1,4,5,4,7,4,7,8,5),
  pag = c(7,6,3,8,3,5,5,6,6.5,4.5,4,8,5,2,8.2,4.3,2.5,7.3,4,6.5,6.5,2),
  jon = c(4,5,5,8,3,5,4,6,4,2,3,7,4,3,5,3,5,6,4,4,6,3),
  kei = c(8,5,4,5,4,6,4,4,8,8,5,6,5,7,5,4,6,5,4,5,6,4),
  dan = c(4,5,8,4,3,5,6,4,4,6,8,8,5,7,6,7,9,6,5,7,4,6),
  jul = c(8,5,6,6,3,7,5,6,8,4,5,8,5,3,7,3,5,6,5,7,6,4),
  kor = c(6,6.5,7.5,4.5,2.5,4.5,5,5,6.5,2.5,7.5,4.5,6,3,5,7,5.5,6.5,6,6.5,6,5),
  cap = c(5,7,9,6,6,6,7,6,8,3,8,5,6,7,6,6,8,5,6,5,3,6),
  jen = c(5,6,8,5,4,6,5,7,8,6,6,5,3,4,5,6,9,5,4,6,6,3),
  chr = c(2,9,4,5,1,8,4,5,6,8,3,5,1,8,6,4,9,6,1,7,6,2)
)

# Enter in the beer information
beers <- data.frame(
  beerid = c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k"),
  name = c("Lost Coast 8-ball", "Southern Tier Chocolate", "Duclaw 865 Coffee",
           "Guiness Extra", "North Coast Old Rasputin Imperial",
           "Duck Rabbit Milk", "Great Divide Yeti Imperial", 
           "Olde Hickory Hickory Stick", "Left Hand Milk",
           "New Holland Dragon's Milk Bourbon Barrel Aged", 
           "Left Hand Bittersweet Imperial Coffee"),
  raw_price = c(1.99,8.99,1.99,1.25,3.49,1.5,9.99,2.79,2.49,7.99,4.49),
  size = c(12, 22, 12, 11.2, 12, 12, 22, 12, 12, 22, 12),
  abv = c(5.8,11,6.9,5.6,9,5.7,9.5,5,6,11,8.9)
) %>% 
  mutate(price = raw_price/size)

# Convert the raw data into a more tidyverse-friendly version (long not wide)
raw_long <- raw %>% 
  gather(key = taster, value = score, -pos, -beerid)
```

# Intro
Here we are again. For those that joined in this experiment for the first time, thank you so much for participating! Your time and opinion are much appreciated. For those who joined in this experiment for the last time, good riddance. Your opinion is worthless and I hate you. Just kidding, but I _do_ want to know why you don't want to be my friend anymore.

In case some of you have forgotten or were only pretending to understand I'll start with a quick recap. All 10 of us sampled 11 different beers. The beers were double blind enough, with all labels covered and the pourers chosen for their apathy towards beer in general. We rated each beer 2 separate times in an attempt to prevent taste fatigue from helping or punishing beer served later in the evening. We'll be taking a look at how each beer fared and how consistent each taster was.

# Meet the Tasters
Unlike the first two times we did this pretty much everyone had a nice distribution of scores. Keith took over Jonathan's role as "one who likes beer", but even then it's not as extreme. Christopher's spread in particular is pretty impressive, ranging from what he would call "I can taste a semblence of hops" (score=1) to "freeze this and it would be a milkshake" (score=9). I'll standardize these to be from 1 to 10 regardless, but it's nice to at least look at the raw spread before I do so.

```{r}
ggplot(raw_long) +
  geom_violin(aes(x = taster, y = score, fill = taster), bw = 0.5) +
  scale_y_continuous(breaks = 1:10, minor_breaks = 1:10) +
  labs(x = "Tasters", y = "Scores",
       title = "Tasters' Score Distributions") +
  theme_minimal() +
  theme(legend.position = "none")
```

# Who Won?
Obviously there are no winners and losers here, except that some people did objectively better than others. If you simply sort by the average difference between scores for the same beer then here's your list:
```{r}
raw_long %>%
  group_by(taster, beerid) %>% 
  summarize(d = max(score)-min(score)) %>% 
  group_by(taster) %>% 
  summarize(mean = mean(d)) %>%
  arrange(mean) %>% select(Taster = taster, `Average Difference` = mean) %>% 
kable(digits = 1, caption = "Difference in Scores for the Same Beer")
```

Keith wins! Christopher and Capen lose! Huzzah! *But wait!* This doesn't seem quite fair. Yes, Keith was most consistent within each beer, but he was also most consistent across *different* beers. Why should he be rewarded for giving every beer a score between 4 and 8, while Christopher is punished for having scores between 1 and 9? Let's standardize and try again.
```{r}
std_long <- raw_long %>% 
  group_by(taster) %>% 
  mutate(score_std = (score-min(score))/(max(score)-min(score))*9 + 1)

std_long %>%
  group_by(taster, beerid) %>% 
  summarize(d = max(score_std)-min(score_std)) %>% 
  group_by(taster) %>% 
  summarize(mean = mean(d), min = min(d), max = max(d)) %>%
  arrange(mean) %>% 
kable(digits = 1, caption = "Difference in Scores for the same Beer")
```

What a twist!! After standardizing, **Keith** wins, and **Capen** loses! As anti-climactic as that was for you to read, just imagine what it was like having to actually run the analysis to get those results. At least Christopher's name was cleared so it wasn't a total waste of time. As fun as it is to crown winners and mock losers, we were all actually pretty similar. What I really wanted to know is whether or not, as a group, we could meaningfully differentiate between beers.

# Perceptive Pallettes
We'll test how well we differentiate beers in what I hope is an intuitive way. Each beer got 2 scores from each of the 10 tasters resulting in 10 pairs of scores per beer. From there we simply take the difference within each pair. The table below demonstrates this for one beer (note scores have been standardized):
```{r}
std_long %>% filter(beerid == "a") %>% select(pos, taster, score_std) %>%
  spread(pos, score_std) %>% mutate(`First Score` = `1`, `Second Score` = `15`,
                                    Difference = abs(`1` - `15`)) %>% 
  select(Taster = taster, `First Score`, `Second Score`, Difference) %>% 
kable(digits = 1, caption = "Lost Coast 8-ball")
```

The average of all those differences is 2.6, which will be the final score for the Lost Coast 8-ball Stout. We can do the same thing for each beer, generating one score per beer. The higher the score, the worse we are at identifying that beer. Next is the fun part, something we in the data science biz like to call "Target Shuffling". For each person we'll reassign their scores at random to different beers, rendering the scores meaningless. Then we can recalculate the same final scores for each beer. Doing this once for the Lost Coast 8-ball Stout we get 2.3 (which is notably less than 2.6). Yikes, that means that if each person had just assigned scores at random we may have had less variability in Lost Coast 8-Ball Stout scores than our actual non-randomized scores. But of course we aren't going to just randomize scores once. We'll do it 10,000 times and for each beer tally up how often randomness was more consistent than reality.

```{r, include=F}
set.seed(1)
std_long %>% group_by(taster) %>% mutate(rscore = sample(score_std)) %>% 
  filter(beerid == "a") %>% select(pos, taster, rscore) %>%
  spread(pos, rscore) %>% mutate(`First Score` = `1`, `Second Score` = `15`,
                                 Difference = abs(`1` - `15`)) %>%
  {mean(.$Difference)}

#ts <- map_df(1:10000, ~{
#  std_long %>% group_by(taster) %>% mutate(rscore = sample(score_std)) %>% 
#    group_by(taster, beerid) %>% 
#    summarize(diff = abs(first(rscore)-last(rscore))) %>%
#    group_by(beerid) %>% summarize(m = mean(diff))
#})
# Save ts so we don't need to run this every time
#save(ts, file = "ts.Rda")
load("/Users/mdbrannock/Documents/Projects/Stouts/ts.Rda")

ts %>% group_by(beerid) %>% summarize(`Mean Random Score` = mean(m))

# Save how often each beer beats randomness
cons_scores <- std_long %>% group_by(taster, beerid) %>% 
  summarize(diff = abs(first(score_std) - last(score_std))) %>% 
  group_by(beerid) %>% summarize(actual = mean(diff))

pvalues <- map_df(letters[1:11], ~{
  temp_ts <- ts %>% filter(beerid == .x) %>% {.$m}
  temp_cs <- cons_scores %>% filter(beerid == .x) %>% {.$actual}
  tibble(beerid = .x, `P-value` = 1-sum(temp_cs<temp_ts)/length(temp_ts))
})
```

```{r}
std_long %>% group_by(beerid) %>% summarize(`Avg. Score` = mean(score_std)) %>%
  left_join(pvalues, by = "beerid") %>% 
  left_join(beers, by = "beerid") %>% 
  mutate(`Price/12 oz` = price*12) %>% 
  select(Beer = name, `P-value`, `Avg. Score`, `Price/12 oz`) %>% 
  arrange(`Avg. Score`) %T>% {pvalues.2 <<- .} %>% 
kable(digits = 2, caption = "Consistencies and Average Scores for Each Beer")
```

For those who don't remember their statistics, a p-value is the probably that an event happened assuming some "null hypothesis" is true. In this case, our p-value is the probability that random scores were more consistent for a beer than our true scores. A lower p-value means that we were very consistent in our judgement of that beer, while a high p-value means the opposite. As can be seen above, there are really just a handful of beers that we were truly consistent in judging. What's even more depressing is that, with the exception of Guiness, those also happen to be the beers we _didn't like_. You can also see in Table 4 which beers did best and worst overall. I didn't do any score adjustments due to price, so their price per 12 fluid ounces is also included. It didn't seem like any adjustments were necessary given that the cheaper beers pretty much already win.

# But What Did *I* Like?
We've established that as a group we're not very good at this, but that shouldn't stop us from figuring out which beers each of us liked best. 

```{r}

```

