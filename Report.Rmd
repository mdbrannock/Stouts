---
title: "This Time With Stouts"
author: "Daniel Brannock"
date: "1/27/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
library(knitr)
library(tidyverse)
library(magrittr)

options(stringsAsFactors = F)
```

```{r data entry}
# Record the data. Each person tasted 22 samples.
raw <- data.frame(
  pos = 1:22,
  beerid = c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k",
             "f", "h", "j", "a", "c", "b", "d", "g", "k", "i", "e"),
  Victoria = c(7,3,5,6,2,4,5,3,6,1,5,6,2,1,4,5,4,7,4,7,8,5),
  Page = c(7,6,3,8,3,5,5,6,6.5,4.5,4,8,5,2,8.2,4.3,2.5,7.3,4,6.5,6.5,2),
  Jonathan = c(4,5,5,8,3,5,4,6,4,2,3,7,4,3,5,3,5,6,4,4,6,3),
  Keith = c(8,5,4,5,4,6,4,4,8,8,5,6,5,7,5,4,6,5,4,5,6,4),
  Daniel = c(4,5,8,4,3,5,6,4,4,6,8,8,5,7,6,7,9,6,5,7,4,6),
  Julie = c(8,5,6,6,3,7,5,6,8,4,5,8,5,3,7,3,5,6,5,7,6,4),
  Kori = c(6,6.5,7.5,4.5,2.5,4.5,5,5,6.5,2.5,7.5,4.5,6,3,5,7,5.5,6.5,6,6.5,6,5),
  Capen = c(5,7,9,6,6,6,7,6,8,3,8,5,6,7,6,6,8,5,6,5,3,6),
  Jennifer = c(5,6,8,5,4,6,5,7,8,6,6,5,3,4,5,6,9,5,4,6,6,3),
  Christopher = c(2,9,4,5,1,8,4,5,6,8,3,5,1,8,6,4,9,6,1,7,6,2)
)

# Enter in the beer information
beers <- data.frame(
  beerid = c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k"),
  name = c("Lost Coast 8-ball", "Southern Tier Chocolate", "Duclaw 865 Coffee",
           "Guiness Extra", "North Coast Old Rasputin Imperial",
           "Duck Rabbit Milk", "Great Divide Yeti Imperial", 
           "Olde Hickory Hickory Stick", "Left Hand Milk",
           "New Holland Dragon's Milk Bourbon Barrel Aged", 
           "Left Hand Bittersweet Imperial Coffee"),
  raw_price = c(1.99,8.99,1.99,1.25,3.49,1.5,9.99,2.79,2.49,7.99,4.49),
  size = c(12, 22, 12, 11.2, 12, 12, 22, 12, 12, 22, 12),
  abv = c(5.8,11,6.9,5.6,9,5.7,9.5,5,6,11,8.9)
) %>% 
  mutate(price = raw_price/size)

# Convert the raw data into a more tidyverse-friendly version (long not wide)
raw_long <- raw %>% 
  gather(key = taster, value = score, -pos, -beerid)
```

# Intro
Here we are again. For those that joined in this experiment for the first time, thank you so much for participating! Your time and opinion are much appreciated. For those who joined in this experiment for the last time, good riddance. Your opinion is worthless and I hate you. Just kidding, but I _do_ want to know why you don't want to be my friend anymore.

In case some of you have forgotten or were only pretending to understand I'll start with a quick recap of what we did. All 10 of us sampled 11 different beers. The beers were double blind enough, with all labels covered and the pourers chosen for their apathy towards beer in general. We rated each beer 2 separate times in an attempt to prevent taste fatigue from helping or punishing beer served later in the evening. We'll be taking a look at how each beer fared and how consistent each taster was.

# Meet the Tasters
Unlike the first two times we did this pretty much everyone had a nice distribution of scores. Keith took over Jonathan's role as "one who likes beer", but even then it's not as extreme. Christopher's spread in particular is pretty impressive, ranging from what he would call "I can taste a semblence of hops" (score=1) to "freeze this and it would be a milkshake" (score=9). I'll standardize these to be from 1 to 10 regardless, but it's nice to at least look at the raw spread before I do so.

```{r}
ggplot(raw_long) +
  geom_violin(aes(x = taster, y = score, fill = taster), bw = 0.5) +
  scale_y_continuous(breaks = 1:10, minor_breaks = 1:10) +
  labs(x = "Tasters", y = "Scores",
       title = "Tasters' Score Distributions") +
  theme_minimal() +
  theme(legend.position = "none")
```

# Who Won?
Obviously there are no winners and losers here, except that some people did objectively better than others. After standardizing the scores, if you simply sort by the average difference between scores for the same beer then here's your list:
```{r}
raw_long %>%
  group_by(taster, beerid) %>% 
  summarize(d = max(score)-min(score)) %>% 
  group_by(taster) %>% 
  summarize(mean = mean(d)) %>%
  arrange(mean) %>% select(Taster = taster, `Average Difference` = mean) %>% 
kable(digits = 1, caption = "Difference in Scores for the Same Beer")
```

Keith wins! Christopher and Capen lose! Huzzah! *But wait!* This doesn't seem quite fair. Yes, Keith was most consistent within each beer, but he was also most consistent across *different* beers. Why should he be rewarded for giving every beer a score between 4 and 8, while Christopher is punished for having scores between 1 and 9? Let's standardize and try again.
```{r}
std_long <- raw_long %>% 
  group_by(taster) %>% 
  mutate(score_std = (score-min(score))/(max(score)-min(score))*9 + 1) %>% 
  ungroup()

std_long %>%
  group_by(taster, beerid) %>% 
  summarize(d = max(score_std)-min(score_std)) %>% 
  group_by(taster) %>% 
  summarize(mean = mean(d), min = min(d), max = max(d)) %>%
  arrange(mean) %>% 
kable(digits = 1, caption = "Difference in Scores for the same Beer")
```

What a twist!! After standardizing, **Keith** wins, and **Capen** loses! But this still doesn't sit right with me (and not because I'm second from last). Even with standardized scores, there is still a possibility that our particular score distributions lend themselves to a high or low average differences in beer ratings. If you don't know what I mean, then I beg you to just trust me. Fortunately we can account for this, but you'll have to bear with me for a second to explain it.

The basic idea is to figure out how often each of us did a better job consistently rating beers than if we drew from our scores at random. For instance, consider Jonathan's standardized scores across all the beers:
```{r}
std_long %>% 
  filter(taster == "Jonathan") %>% 
  left_join(select(beers, beerid, name), by = "beerid") %>% 
  select(name, score = score_std) %>% 
  group_by(name) %>% mutate(pos = row_number()) %>% 
  spread(pos, score) %>% select(Beer = name, First = `1`, Second = `2`) %>% 
  mutate(Difference = abs(First - Second)) %>%
kable(digits = 1, caption = "Jonathan's Scores for All Beers")
```

If you take the average of all those differences then you get 1.8. Now we're going to do something weird. We'll randomly shuffle all of Jonathan's scores around. Before he gave Guiness Extra a perfect 10 for the first tasting then a 7 for the second. We'll randomly reassign that 10 and 7 to (probably) different beers. For example, this is one possible iteration:
```{r}
set.seed(9001)
std_long %>% 
  filter(taster == "Jonathan") %>% 
  left_join(select(beers, beerid, name), by = "beerid") %>% 
  select(name, score = score_std) %>%
  mutate(score = sample(score)) %>% 
  group_by(name) %>% mutate(pos = row_number()) %>% 
  spread(pos, score) %>% select(Beer = name, First = `1`, Second = `2`) %>% 
  mutate(Difference = abs(First - Second)) %>%
kable(digits = 1, caption = "Jonathan's Scores for All Beers")
```

All the scores have been randomly reassigned (you can see the 10 is now assigned to Duclas 865 Coffee). If we take the average of these differences we get 2.6, which is higher than Jonathan's original average of 1.8. But it's 2018, people. We live in a world of big data. Talkin' about the internet of things, machine learning, ARTIFICIAL INTELLIGENCE. I hope I'm making myself clear. We're not going to shuffle these scores once. We're going to do it 10,000 times. Then we'll see how Jonathan's 1.8 holds up compared to all those random iterations.
```{r, include=F}
# target shuffling for each beer across all tasters
#set.seed(1)
#ts_beers <- map_df(1:10000, ~{
#  std_long %>% group_by(taster) %>% mutate(rscore = sample(score_std)) %>% 
#    group_by(taster, beerid) %>% 
#    summarize(diff = abs(first(rscore)-last(rscore))) %>%
#    group_by(beerid) %>% summarize(m = mean(diff))
#})
# Save ts so we don't need to run this every time
#save(ts_beers, 
#     file = "/Users/mdbrannock/Documents/Projects/Stouts/ts_beers.Rda")
load("/Users/mdbrannock/Documents/Projects/Stouts/ts_beers.Rda")

# Target shuffling for each taster across all beers
#set.seed(9001)
#ts_tasters <- map_df(1:10000, ~{
#  std_long %>% group_by(taster) %>% mutate(rscore = sample(score_std)) %>% 
#    group_by(taster, beerid) %>% 
#    summarize(diff = abs(first(rscore)-last(rscore))) %>%
#    group_by(taster) %>% summarize(m = mean(diff))
#})
# Save ts so we don't need to run this every time
#save(ts_tasters, 
#     file = "/Users/mdbrannock/Documents/Projects/Stouts/ts_tasters.Rda")
load("/Users/mdbrannock/Documents/Projects/Stouts/ts_tasters.Rda")

# Save how often each beer beats randomness
cons_scores <- std_long %>% group_by(taster, beerid) %>% 
  summarize(diff = abs(first(score_std) - last(score_std))) %>% 
  group_by(beerid) %>% summarize(actual = mean(diff))

pvalues_beers <- map_df(letters[1:11], ~{
  temp_ts <- ts_beers %>% filter(beerid == .x) %>% {.$m}
  temp_cs <- cons_scores %>% filter(beerid == .x) %>% {.$actual}
  tibble(beerid = .x, `P-value` = 1-sum(temp_cs<temp_ts)/length(temp_ts))
}) %>% arrange(`P-value`)

# Save how often each taster beets randomness
cons_scores <- std_long %>% group_by(taster, beerid) %>% 
  summarize(diff = abs(first(score_std) - last(score_std))) %>% 
  group_by(taster) %>% summarize(actual = mean(diff))

pvalues_tasters <- map_df(cons_scores$taster, ~{
  temp_ts <- ts_tasters %>% filter(taster == .x) %>% {.$m}
  temp_cs <- cons_scores %>% filter(taster == .x) %>% {.$actual}
  tibble(taster = .x, `P-value` = 1-sum(temp_cs<temp_ts)/length(temp_ts))
}) %>% arrange(`P-value`)

kable(pvalues_tasters, digits = 2, caption = "Final Taster Rankings")
```

For those who don't remember their statistics, a p-value is the probability that an event happened assuming some "null hypothesis" is true. In this case, our p-value is the probability that random scores were more consistent for a taster than their true scores. A lower p-value means that taster was very consistent in their judgement of beers, while a high p-value means the opposite.

These final rankings really aren't that different than our earlier ones, but I think they are the most fair. My apologies to Capen, who must feel he has been picked on quite enough for one report. I agree, so we'll move on.

# Perceptive Pallettes
What happens if we do the same thing for beers that we just finished doing for tasters? Great question! It would essentially tell us how consistently we, as a whole, rated each beer. I know you're tired of reading about method, so here are the results (along with each beer's average standardized score):
```{r}
std_long %>% group_by(beerid) %>% summarize(`Avg. Score` = mean(score_std)) %>%
  left_join(pvalues_beers, by = "beerid") %>% 
  left_join(beers, by = "beerid") %>% 
  mutate(`Price/12 oz` = price*12) %>% 
  select(Beer = name, `P-value`, `Avg. Score`, `Price/12 oz`) %>% 
  arrange(`Avg. Score`) %T>% {pvalues.2 <<- .} %>% 
kable(digits = 2, caption = "Consistencies and Average Scores for Each Beer")
```

In this case a low p-value means we did a good job consistently rating that beer, while a high values means we did a poor job. This table was profoundly disapointing to me. As can be seen above, there are really just a handful of beers that we were truly consistent in judging. What's even more depressing is that, with the exception of Guiness, those also happen to be the beers we _didn't like_. I didn't do any score adjustments due to price, so their price per 12 fluid ounces is also included. It didn't seem like any adjustments were necessary given that the cheaper beers pretty much already win.

# But What Did *I* Like?
We've established that as a group we're not very good at this, but that shouldn't stop us from figuring out which beers each of us liked best. The table below shows the top 3 beers for each taster.

```{r}
# average score and rank for each beer for each person
avgd <- std_long %>% 
  group_by(taster, beerid) %>% 
  summarize(score = mean(score_std)) %>%
  group_by(taster) %>% 
  mutate(r = rank(-score, ties.method = "first")) %>% ungroup()

# Spread this out for the top 3 for each taster 
avgd %>% filter(r <= 3) %>% 
  left_join(beers, by = "beerid") %>% 
  select(taster, name, r) %>% 
  spread(r, name) %>% 
  rename(Taster = taster, Favorite = `1`, Second = `2`, Third = `3`) %>% 
kable(caption = "Everyone Favorite Beers")
```

The last analysis I'll bore you with is with whom your preference in stouts is most closely aligned. Using just the most basic of clustering algorithms, there are 3 pretty clear groups of tasters. 

```{r}
# Convert to a matrix that can actually be used for clustering/PCA
std_wide <- avgd %>% 
  select(taster, beerid, score) %>% 
  spread(taster, score)

# This actually doens't end up being that great
pca <- prcomp(std_wide[, 2:ncol(std_wide)])$rotations %>% as.data.frame()

# Try clustering (which needs a different wide format)
std_wide2 <- avgd %>% 
  select(taster, beerid, score) %>% 
  spread(beerid, score) %T>% {rownames(.) <- .$taster} %>% 
  select(-taster)
d <- dist(std_wide2)
clus <- hclust(d, method = "complete")
plot(clus, xlab = "", main = "Clustered Tasters", ylim = c(2, 15), sub = '')
par(mar = c(2,4,4,2) + 0.1)
```

# In Conclusion
